{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pinterest Data Pipeline Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "1. [Milestone 1: Set up the Environment](#milestone-1-set-up-the-environment)\n",
    "1. [Milestone 2: Get Started](#milestone-2-get-started)\n",
    "1. [Milestone 3: Batch Processing: Configure the EC2 Kafka Client](#milestone-3-batch-processing-configure-the-ec2-kafka-client)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Milestone 1: Set up the Environment\n",
    "1. Set up GitHub\n",
    "1. Set up AWS (This AWS was set up by AI-Core as some of the features are paid services)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Milestone 2: Get Started\n",
    "1. Download the \"user_posting_emulation.py\" file from AI-Core which is a continuos stream of Pinterest data\n",
    "1. Sign into AWS account making sure to be in the correct region (us-east-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Milestone 3: Batch Processing: Configure the EC2 Kafka Client\n",
    "1. Create a key pair file. \n",
    "1. Connect to the EC2 instance using:\n",
    "    - ssh -i \"0af85661a221-key-pair.pem\" ec2-user@ec2-54-158-237-202.compute-1.amazonaws.com\n",
    "1. Set up Kafka on the EC2 instance.\n",
    "    1. sudo yum install java-1.8.0\n",
    "    1. wget https://archive.apache.org/dist/kafka/2.8.1/kafka_2.12-2.8.1.tgz\n",
    "    1. tar -xzf kafka_2.12-2.8.1.tgz\n",
    "    1. Navigate to libs foulder then:\n",
    "        - wget https://github.com/aws/aws-msk-iam-auth/releases/download/v1.1.5/aws-msk-iam-auth-1.1.5-all.jar\n",
    "    1. Add the CLASSPATH variable to .bashrc\n",
    "        - export CLASSPATH=/home/ec2-user/kafka_2.12-2.8.1/libs/aws-msk-iam-auth-1.1.5-all.jar\n",
    "    1. Add my user role to the principle trust policy.\n",
    "        - arn:aws:iam::584739742957:role/0af85661a221-ec2-access-role\n",
    "    1. Configure client.properties in the kafka_folder/bin to use the correct AWS IAM authentication to the cluster.\n",
    "1. Create Kafka Topics using the [Bootstrap servers string] and the [Plaintext Apache Zookeeper] connection string.\n",
    "    - your_UserId.pin for the Pinterest posts data\n",
    "    - your_UserId.geo for the post geolocation data\n",
    "    - your_UserId.user for the post user data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Milestone 4: Batch Processing: Connect an MSK Cluster to an S3 Bucket\n",
    "1. Create a custom plugin with MSK Connect\n",
    "1. Create a connector with MSK Connect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Milestone 5: Batch Processing: Configure an API Gateway\n",
    "1. Build a Kafka REST proxy integration method for the API\n",
    "1. Set up the Kafka REST\n",
    "    - start API Rest proxy with: \n",
    "        1. ssh -i \"0af85661a221-key-pair.pem\" ec2-user@ec2-54-158-237-202.compute-1.amazonaws.com\n",
    "        1. cd confluent-7.2.0/bin\n",
    "        1. ./kafka-rest-start /home/ec2-user/confluent-7.2.0/etc/kafka-rest/kafka-rest.properties\n",
    "1. Send data to the API \n",
    "    - \"user_posting_emulation.py\" is the file that emulates posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Milestone 6: Batch Processing: Databricks\n",
    "1. Set up Databricks account\n",
    "1. write code to access AWS S3 bucket from Databricks saving data as dataframe \"Accessing AWS S3 Bucket from Databricks using IAM role authentication.ipynb\"\n",
    "    - https://dbc-b54c5c54-233d.cloud.databricks.com/?o=1865928197306450#notebook/198588058359791/command/198588058359802\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
